
# Paper Name:
**_Fully-adaptive Feature Sharing in Multi-Task Networks with Applications in Person Attribute Classification_**
# publishing information

# 1. background problem/motivation:

# 2. the proposed methods:

# 3. dataset:

# 4. advantages:

# 5. the detail of methods:

# 6. contribution:

# 7. any questions during the reading :

# 8. vocabulary:
tedious 乏味
compact 紧凑的
possess 具有
seamlessly 无缝
criterion 准则
orthogonal 正交
affinity 亲和度
concurrently 同时
intrinsically 本质上
contradict 矛盾
favor 支持

# 中文翻译
## Abstract
多任务学习目的是通过在任务内合适地分享信息来来提高多个预测任务的性能，，在深度神经网络的背景下，这个想法经常是通过手动设计的神经网络来实现，这些架构具有任务与分支之间共享的层，这些层可以编码特定任务的特征，现在的方法都是手动决定网络结构，不好。所以提出一个自动设计紧凑的多任务学习网络架构。从一个薄的多层网络开始，动态扩展网络的宽度。通过迭代地扩展，它就创建了一个🌲状网络。

## Introduction
介绍人类很强，能自主迁移多任务学习，在多任务学习中有效的分享信息机制很重要
提出了一个新方法，方法有两个特点：
基于分支自动学习多任务的架构，2， 选择性分享，通过自动学习来决定分享，还希望有低延迟与低内存
传统的方法， 前面的层共享参数，后面出现分支，这种方法是很主观的，是基于手动探索作出的决定。如何找到方法去取代手动探索 很重要，而且很少人关注。
提出的方法使用自顶向下的贪婪模式，在使用新的标准在每层进行分支，以及任务分类决策，该标准促进为不相关的任务（或任务组）创建单独的分支，同时惩罚 模型的复杂性，为了解决多任务的规模问题，一开始的时候是非常薄的网络，然后在训练过程中，基于上述标准创建新的分支动态地扩展它。

提出了一种基于同时正交匹配追踪（SOMP的方法，用于初始化来自预训练的更宽网络（例如，VGG-16）的细网络作为该工作中的副作用。
测试了一下，效果很好，参数更少，速度更快

主要贡献：
提出了通过一个新颖的动态分支过程来实现自动学习的多任务深度网络架构，在每个层都做决策，既考虑到任务相关性又考虑到模型复杂性
提出了一个基于同时正交匹配追踪的方法，用于从更广泛的预训练网络模型初始化薄网络，从而实现更快的收敛和更高的准确性
在 脸部属性、衣服属性、联合人物属性中，进行了测试，研究了方法的行为。

## Related Work
多任务学习：
多任务学习的历史很久，一些方法已经解决了每个任务应该与谁共享特征的问题，这些问题主要是对浅层分类模型设计的，而我们的方法研究的是在结构化模型里对不同任务的特征分享。
随着层数增加，通过手工制作的网络分支定义跨任务的正确级别的功能共享是不切实际的
模型压缩跟加速。许多方法可以用
人物属性分类 介绍应用，介绍之前的方法

## Methodology
介绍 前向网络流程，

介绍灵感哪儿的，现在通用的网络，都是倒金字塔型的，高层的数据更倾向于是任务相关的，对高层的宽度进行调整可以取得更好的效果。被这两个观察结果触动，从扁平的架构开始，也就是所有层的通道都是一样的，然后逐步去扩展它，扩展过程如下，
对模型进行初始化，初始化方法有两种，一随机初始化，二是使用提出的SOMP
自适应的模型扩展，方式是自顶向下贪婪地扩展，当不再扩展时，固定模型。
### Thin Networks and Filter Selection using Simultaneous Orthogonal Matching Pursuit

$w$ 设定为模型的通道数，预训练的模型可以加速训练，但是如果 模型不一样就不能抄，如何解决这个问题，提出了这个方法，在文献中，将大的网络的参数抄到任意的小网络中，这种方法叫做 knowledge distillation，

训练方法有点像 数据降维，PCA那种，对于每个层，
最小化 $ | W^{p,l} - AW^{p,l}_{w:}||_F$,其中 $A \in R^{d * d'}， W^{p,l} \in R^{d*d'}, w =d'$,得到 $w$,然后将$W_{w^*}^{p,l}$作为预训练的参数  

### Top-Down Layer-wise Model Widening
介绍了 l 层有d个子网络，如何处理
l-1 层有c个网络，如何计算输出。
还是没介绍重点

### Task Grouping based on the Probability of Concurrently Simple or Difficult Example
定义了两个任务之间的相似度：在同一个随机采样的训练数据中，观察到同时为简单或者是困难样本的概率
为了让这个相似度更具体一点，提出了 困难或者简单的定义，错误的绝对值 $m^n_i = |t^n_i - s^n_i|$, t代表低i个任务第n个样本的真实标签，s 是预测标签，
应该要设置一个会改变的$m^n_i$ ，不然随着训练的过程，这个阈值会越来越低，变得不可靠。
所以使用 $E(m_i)$作为阈值， 当小于这个值，就称为简单样本，大于就称为困难样本，定义 两个任务相似度为 $$A(i,j) = P(e^n_i=1,e^n_j=1) + P(e^n_i=0,e^n_j=0)$$